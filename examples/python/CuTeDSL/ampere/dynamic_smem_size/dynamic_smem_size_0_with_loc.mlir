

!memref_smem_f16_ = !cute.memref<f16, smem, align<64>, "(32,16):(1,32)">
module attributes {gpu.container_module} {
  gpu.module @kernels {
    cuda.kernel @kernel_cutlass_kernel_0() attributes {cu_attrs = {max_dynamic_shared_size_bytes = #cuda.dev_max_shared_memory_optin, non_portable_cluster_size_allowed = 1 : i32}, cute.kernel, gpu.kernel, nvvm.reqntid = array<i32: 1, 1, 1>} {
      %smem_ptr = cute_nvgpu.arch.get_dyn_smem() : !cute.ptr<i8, smem, align<1024>> loc(#loc17)
      %int_tuple = cute.make_int_tuple() : () -> !cute.int_tuple<"264"> loc(#loc18)
      %ptr = cute.add_offset(%smem_ptr, %int_tuple) : (!cute.ptr<i8, smem, align<1024>>, !cute.int_tuple<"264">) -> !cute.ptr<i8, smem, align<8>> loc(#loc18)
      %smem_size = cute_nvgpu.arch.get_dyn_smem_size() : i32 loc(#loc19)
      %c264_i32 = arith.constant 264 : i32 loc(#loc15)
      %0 = arith.cmpi sge, %smem_size, %c264_i32 : i32 loc(#loc20)
      cf.assert %0, "Allocation failed: shared memory allocation exceeds available memory set in kernel launch. Allocated bytes: 264 bytes. Please reduce the allocation or set a larger smem size in kernel launch." loc(#loc19)
      %int_tuple_0 = cute.make_int_tuple() : () -> !cute.int_tuple<"0"> loc(#loc21)
      %ptr_1 = cute.add_offset(%smem_ptr, %int_tuple_0) : (!cute.ptr<i8, smem, align<1024>>, !cute.int_tuple<"0">) -> !cute.ptr<i8, smem, align<1024>> loc(#loc21)
      %int_tuple_2 = cute.make_int_tuple() : () -> !cute.int_tuple<"256"> loc(#loc22)
      %ptr_3 = cute.add_offset(%smem_ptr, %int_tuple_2) : (!cute.ptr<i8, smem, align<1024>>, !cute.int_tuple<"256">) -> !cute.ptr<i8, smem, align<256>> loc(#loc22)
      %iter = cute.recast_iter(%ptr_3) : !cute.ptr<i8, smem, align<256>> to !cute.ptr<i32, smem, align<256>> loc(#loc22)
      %int_tuple_4 = cute.make_int_tuple() : () -> !cute.int_tuple<"260"> loc(#loc22)
      %ptr_5 = cute.add_offset(%smem_ptr, %int_tuple_4) : (!cute.ptr<i8, smem, align<1024>>, !cute.int_tuple<"260">) -> !cute.ptr<i8, smem, align<4>> loc(#loc22)
      %iter_6 = cute.recast_iter(%ptr_5) : !cute.ptr<i8, smem, align<4>> to !cute.ptr<i8, smem, align<4>> loc(#loc22)
      %1 = cute.ptrtoint(%ptr) : !cute.ptr<i8, smem, align<8>> to i32 loc(#loc23)
      %c64_i32 = arith.constant 64 : i32 loc(#loc15)
      %2 = arith.addi %1, %c64_i32 : i32 loc(#loc24)
      %c1_i32 = arith.constant 1 : i32 loc(#loc15)
      %3 = arith.subi %2, %c1_i32 : i32 loc(#loc24)
      %c-64_i32 = arith.constant -64 : i32 loc(#loc15)
      %4 = arith.andi %3, %c-64_i32 : i32 loc(#loc24)
      %5 = arith.extsi %4 : i32 to i64 loc(#loc15)
      %iv = cute.assume(%5) : (i64) -> !cute.i64<divby 64> loc(#loc23)
      %6 = cute.inttoptr(%iv) : !cute.i64<divby 64> to !cute.ptr<i8, smem, align<64>> loc(#loc23)
      %int_tuple_7 = cute.make_int_tuple() : () -> !cute.int_tuple<"512"> loc(#loc18)
      %ptr_8 = cute.add_offset(%6, %int_tuple_7) : (!cute.ptr<i8, smem, align<64>>, !cute.int_tuple<"512">) -> !cute.ptr<i8, smem, align<64>> loc(#loc18)
      %smem_size_9 = cute_nvgpu.arch.get_dyn_smem_size() : i32 loc(#loc25)
      %c832_i32 = arith.constant 832 : i32 loc(#loc15)
      %7 = arith.cmpi sge, %smem_size_9, %c832_i32 : i32 loc(#loc20)
      cf.assert %7, "Allocation failed: shared memory allocation exceeds available memory set in kernel launch. Allocated bytes: 832 bytes. Please reduce the allocation or set a larger smem size in kernel launch." loc(#loc25)
      %int_tuple_10 = cute.make_int_tuple() : () -> !cute.int_tuple<"512"> loc(#loc18)
      %ptr_11 = cute.add_offset(%ptr_8, %int_tuple_10) : (!cute.ptr<i8, smem, align<64>>, !cute.int_tuple<"512">) -> !cute.ptr<i8, smem, align<64>> loc(#loc18)
      %smem_size_12 = cute_nvgpu.arch.get_dyn_smem_size() : i32 loc(#loc26)
      %c1344_i32 = arith.constant 1344 : i32 loc(#loc15)
      %8 = arith.cmpi sge, %smem_size_12, %c1344_i32 : i32 loc(#loc20)
      cf.assert %8, "Allocation failed: shared memory allocation exceeds available memory set in kernel launch. Allocated bytes: 1344 bytes. Please reduce the allocation or set a larger smem size in kernel launch." loc(#loc26)
      %iter_13 = cute.recast_iter(%ptr_8) : !cute.ptr<i8, smem, align<64>> to !cute.ptr<i32, smem, align<64>> loc(#loc26)
      %shape = cute.make_shape() : () -> !cute.shape<"(32,16)"> loc(#loc27)
      %lay = cute.make_layout(%shape) : !cute.layout<"(32,16):(1,32)"> loc(#loc27)
      %coord = cute.make_coord() : () -> !cute.coord<"0"> loc(#loc28)
      %idx = cute.crd2idx(%coord, %lay) : (!cute.coord<"0">, !cute.layout<"(32,16):(1,32)">) -> !cute.int_tuple<"0"> loc(#loc28)
      %e0 = cute.get_leaves(%idx) : !cute.int_tuple<"0"> loc(#loc28)
      %cosz = cute.cosize(%lay) : (!cute.layout<"(32,16):(1,32)">) -> !cute.int_tuple<"512"> loc(#loc28)
      %e0_14 = cute.get_leaves(%cosz) : !cute.int_tuple<"512"> loc(#loc28)
      %int_tuple_15 = cute.make_int_tuple() : () -> !cute.int_tuple<"1024"> loc(#loc18)
      %ptr_16 = cute.add_offset(%ptr_11, %int_tuple_15) : (!cute.ptr<i8, smem, align<64>>, !cute.int_tuple<"1024">) -> !cute.ptr<i8, smem, align<64>> loc(#loc18)
      %smem_size_17 = cute_nvgpu.arch.get_dyn_smem_size() : i32 loc(#loc28)
      %c2368_i32 = arith.constant 2368 : i32 loc(#loc15)
      %9 = arith.cmpi sge, %smem_size_17, %c2368_i32 : i32 loc(#loc20)
      cf.assert %9, "Allocation failed: shared memory allocation exceeds available memory set in kernel launch. Allocated bytes: 2368 bytes. Please reduce the allocation or set a larger smem size in kernel launch." loc(#loc28)
      %iter_18 = cute.recast_iter(%ptr_11) : !cute.ptr<i8, smem, align<64>> to !cute.ptr<f16, smem, align<64>> loc(#loc28)
      %view = cute.make_view(%iter_18, %lay) : !memref_smem_f16_ loc(#loc28)
      %iter_19 = cute.get_iter(%view) : !memref_smem_f16_ loc(#loc29)
      return loc(#loc15)
    } loc(#loc16)
  } loc(#loc15)
  func.func @cutlass_launch_kernel1() -> i32 attributes {llvm.emit_c_interface} {
    %c2368_i32 = arith.constant 2368 : i32 loc(#loc15)
    %c0_i64 = arith.constant 0 : i64 loc(#loc15)
    %0 = cuda.cast %c0_i64 : i64 -> !cuda.stream loc(#loc16)
    %1 = arith.extsi %c2368_i32 : i32 to i64 loc(#loc15)
    %c1_i32 = arith.constant 1 : i32 loc(#loc15)
    %2 = cuda.launch_cfg.create<max_attrs = 2 : i32> (blockDim = (%c1_i32, %c1_i32, %c1_i32), dynamicSmemBytes = %1, gridDim = (%c1_i32, %c1_i32, %c1_i32), stream = %0) : i32, i32, i32, i64, i32, i32, i32, !cuda.stream -> !cuda.launch_cfg<max_attrs = 2> loc(#loc16)
    %3 = cuda.launch_ex @kernels::@kernel_cutlass_kernel_0<%2> () : !cuda.launch_cfg<max_attrs = 2>, () -> !cuda.result loc(#loc16)
    %4 = cuda.cast %3 : !cuda.result -> i32 loc(#loc16)
    cuda.return_if_error %4 : i32 loc(#loc16)
    %c0_i32 = arith.constant 0 : i32 loc(#loc15)
    return %c0_i32 : i32 loc(#loc15)
  } loc(#loc15)
} loc(#loc15)
#loc = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":119:0)
#loc1 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":104:0)
#loc2 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":67:0)
#loc3 = loc("/usr/local/lib/python3.10/dist-packages/nvidia_cutlass_dsl/python_packages/cutlass/utils/smem_allocator.py":178:0)
#loc4 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":70:0)
#loc5 = loc("/usr/local/lib/python3.10/dist-packages/nvidia_cutlass_dsl/python_packages/cutlass/utils/smem_allocator.py":187:0)
#loc6 = loc("/usr/local/lib/python3.10/dist-packages/nvidia_cutlass_dsl/python_packages/cutlass/cute/core.py":4069:0)
#loc7 = loc("/usr/local/lib/python3.10/dist-packages/nvidia_cutlass_dsl/python_packages/cutlass/cute/core.py":4066:0)
#loc8 = loc("/usr/local/lib/python3.10/dist-packages/nvidia_cutlass_dsl/python_packages/cutlass/utils/smem_allocator.py":176:0)
#loc9 = loc("/usr/local/lib/python3.10/dist-packages/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/typing.py":846:0)
#loc10 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":71:0)
#loc11 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":72:0)
#loc12 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":75:0)
#loc13 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":73:0)
#loc14 = loc("/usr/local/lib/python3.10/dist-packages/nvidia_cutlass_dsl/python_packages/cutlass/_mlir/dialects/_cute_ops_gen.py":2415:0)
#loc15 = loc("cute.compile(launch_kernel1)"(#loc))
#loc16 = loc("k.launch("(#loc1))
#loc17 = loc("allocator = cutlass.utils.SmemAllocator()"(#loc2))
#loc18 = loc("self._base += size_in_bytes"(#loc3))
#loc19 = loc("shared_data = allocator.allocate(SharedData)"(#loc4))
#loc20 = loc("self._allocated_bytes <= get_dyn_smem_size(loc=loc, ip=ip),"(#loc5))
#loc21 = loc("new_obj = struct._MemRangeData(obj._dtype, obj._size, base + off)"(#loc6))
#loc22 = loc("new_obj = recast_ptr(base + off, dtype=obj)"(#loc7))
#loc23 = loc("self._base = self._base.align(byte_alignment)"(#loc8))
#loc24 = loc("res_val = op(lhs_val, rhs_val)"(#loc9))
#loc25 = loc("raw_buffer = allocator.allocate(512, byte_alignment=64)"(#loc10))
#loc26 = loc("int_array = allocator.allocate_array(element_type=cutlass.Int32, num_elems=128)"(#loc11))
#loc27 = loc("layout=cute.make_layout((32, 16)),"(#loc12))
#loc28 = loc("tensor_smem = allocator.allocate_tensor("(#loc13))
#loc29 = loc("return self.operation.results[0]"(#loc14))



module attributes {gpu.container_module} {
  gpu.module @kernels {
    cuda.kernel @kernel_cutlass_kernel_no_smem_0() attributes {cu_attrs = {max_dynamic_shared_size_bytes = #cuda.dev_max_shared_memory_optin, non_portable_cluster_size_allowed = 1 : i32}, cute.kernel, gpu.kernel, nvvm.reqntid = array<i32: 1, 1, 1>} {
      %0 = nvvm.read.ptx.sreg.ctaid.x : i32 loc(#loc7)
      %1 = nvvm.read.ptx.sreg.ctaid.y : i32 loc(#loc7)
      %2 = nvvm.read.ptx.sreg.ctaid.z : i32 loc(#loc7)
      %c0_i32 = arith.constant 0 : i32 loc(#loc5)
      %3 = arith.cmpi eq, %0, %c0_i32 : i32 loc(#loc8)
      scf.if %3 {
        cute.print("Hello world\0A", ) :  loc(#loc9)
      } loc(#loc5)
      return loc(#loc5)
    } loc(#loc6)
  } loc(#loc5)
  func.func @cutlass_launch_kernel2() -> i32 attributes {llvm.emit_c_interface} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc5)
    %c0_i64 = arith.constant 0 : i64 loc(#loc5)
    %0 = cuda.cast %c0_i64 : i64 -> !cuda.stream loc(#loc6)
    %1 = arith.extsi %c0_i32 : i32 to i64 loc(#loc5)
    %c1_i32 = arith.constant 1 : i32 loc(#loc5)
    %2 = cuda.launch_cfg.create<max_attrs = 2 : i32> (blockDim = (%c1_i32, %c1_i32, %c1_i32), dynamicSmemBytes = %1, gridDim = (%c1_i32, %c1_i32, %c1_i32), stream = %0) : i32, i32, i32, i64, i32, i32, i32, !cuda.stream -> !cuda.launch_cfg<max_attrs = 2> loc(#loc6)
    %3 = cuda.launch_ex @kernels::@kernel_cutlass_kernel_no_smem_0<%2> () : !cuda.launch_cfg<max_attrs = 2>, () -> !cuda.result loc(#loc6)
    %4 = cuda.cast %3 : !cuda.result -> i32 loc(#loc6)
    cuda.return_if_error %4 : i32 loc(#loc6)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc5)
    return %c0_i32_0 : i32 loc(#loc5)
  } loc(#loc5)
} loc(#loc5)
#loc = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":120:0)
#loc1 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":113:0)
#loc2 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":88:0)
#loc3 = loc("/usr/local/lib/python3.10/dist-packages/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/typing.py":846:0)
#loc4 = loc("/home/jayce.su/notes/dsl/cutlass_with_notes/examples/python/CuTeDSL/ampere/dynamic_smem_size.py":90:0)
#loc5 = loc("cute.compile(launch_kernel2)"(#loc))
#loc6 = loc("k.launch("(#loc1))
#loc7 = loc("tidx, _, _ = cute.arch.block_idx()"(#loc2))
#loc8 = loc("res_val = op(lhs_val, rhs_val)"(#loc3))
#loc9 = loc("cute.printf(\22Hello world\22)"(#loc4))

