{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31330d3",
   "metadata": {},
   "source": [
    "# Custom epilogue fusions for GEMMs\n",
    "\n",
    "Note: this notebook requires a GPU with compute capability 100 or 103:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb450878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cutlass_api\n",
    "\n",
    "if not (status := cutlass_api.utils.is_device_cc_supported({100, 103})):\n",
    "    print(f\"This notebook requires a GPU with compute capability 100 or 103.\\n{status.error}\")\n",
    "    import sys\n",
    "\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e9d59",
   "metadata": {},
   "source": [
    "The CUTLASS API provides flexible epilogue fusion support by allowing for the specification of an epilogue via high-level tensor operations that one would like to compose with an operation.\n",
    "\n",
    "For those familiar with the legacy CUTLASS Python API's [epilogue visitor tree frontend](https://github.com/NVIDIA/cutlass/blob/a2439551c765c5393aebe557ee75d3a0412d2211/examples/python/deprecated/04_epilogue_visitor.ipynb), much of the interface is shared.\n",
    "\n",
    "The CUTLASS API enables one to express an epilogue using a function operating at the `torch.Tensor`-level, and has tooling to automatically add this to kernels supporting the provided function. \n",
    "\n",
    "For example, in PyTorch one might write the following to compute a GEMM + epilogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d77d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(2025)\n",
    "\n",
    "L, M, N, K = 1, 1024, 1024, 1024\n",
    "A = torch.randn(L, M, K, device=\"cuda\", dtype=torch.float16)\n",
    "B = torch.randn(L, K, N, device=\"cuda\", dtype=torch.float16)\n",
    "C = torch.randn(L, M, N, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "def my_epilogue(accum, C, alpha, beta, extra_scalar):\n",
    "    Aux = (alpha * accum) + (beta * C)\n",
    "    D = extra_scalar * Aux\n",
    "    return D, Aux\n",
    "\n",
    "alpha, beta, extra_scalar = 1.0, 2.0, 0.5\n",
    "D, Aux = my_epilogue(A @ B, C, alpha, beta, extra_scalar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee4dd1",
   "metadata": {},
   "source": [
    "The CUTLASS API allows the same epilogue function `my_epilogue` to be used in GEMMs provided by the API.\n",
    "\n",
    "To do so, one defines `EpilogueArguments` consisting of the epilogue function to compute (or a string representation of it) along with arguments corresponding to each input and output of the function (except for `accum`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cutlass_api\n",
    "from cutlass_api.arguments import GemmArguments, EpilogueArguments\n",
    "\n",
    "# Allocate buffers for D and Aux\n",
    "D_, Aux_ = [torch.empty((L, M, N), device=\"cuda\", dtype=torch.float16) for _ in range(2)]\n",
    "\n",
    "epi_args = EpilogueArguments(my_epilogue, C=C, alpha=alpha, beta=beta, extra_scalar=extra_scalar, D=D_, Aux=Aux_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef8e8a",
   "metadata": {},
   "source": [
    "These arguments can be added to `GemmArguments` and passed in to `get_kernels()` for use when retrieving compatible kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60215c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = GemmArguments(A=A, B=B, out=D, accumulator_type=torch.float32, epilogue=epi_args)\n",
    "kernels = cutlass_api.get_kernels(args)\n",
    "assert len(kernels) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7f9a2",
   "metadata": {},
   "source": [
    "Each of the kernels returned by `get_kernels` can be compiled and executed just the same with these new arguments, as it was in examples without\n",
    "epilogue fusion. For example, using the first kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels[0].run(args)\n",
    "\n",
    "torch.testing.assert_close(D, D_)\n",
    "torch.testing.assert_close(Aux, Aux_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a826e3",
   "metadata": {},
   "source": [
    "## How the epilogue fusion API works\n",
    "To support specifying an epilogue via a Python function, a kernel needs some mechanism to:\n",
    "1. Detect the operations in the epilogue function\n",
    "2. Determine if the kernel can support the operations\n",
    "3. Emit code to perform these operations within the kernel\n",
    "\n",
    "Step 1 listed above does not depend on the kernel and its implementation (e.g., DSL), while steps 2 and 3 depend on the kernel and/or its implementation.\n",
    "\n",
    "Thus, the CUTLASS API separates these components so that step 1 takes place at the API level and steps 2 and 3 take place in the kernel. This process is visualized below. We will walk through each step in greater detail.\n",
    "\n",
    "```python\n",
    "    +------------------------------------+\n",
    "    | def epi(accum, alpha, beta, C):    |\n",
    "    |   D = (accum * alpha) + (beta * C) |      1. Define epilogue via a Python function\n",
    "    |   return D                         |\n",
    "    +------------------------------------+\n",
    "                      |\n",
    "                      |\n",
    "                      |\n",
    "   GemmArguments(...,                           2. Pass epilogue function, operands, and outputs\n",
    "     epilogue=EpilogueArguments(                   to EpilogueArguments constructor,\n",
    "        epi, alpha=alpha, beta=beta, C=C))         and add this to the GemmArguments. Under the\n",
    "                      |                            hood, this parses the Python AST of the\n",
    "                      |                            epilogue function to produce a DAG of load,\n",
    "                      |                            store, and compute nodes.\n",
    "                      V\n",
    "  +-----------------------------------------+ \n",
    "  |      Intermediate DAG representation    |\n",
    "  |      ===============================    |\n",
    "  |                                         |\n",
    "  |                Store()                  |\n",
    "  |                   |                     |\n",
    "  |                 Add()                   |\n",
    "  |                 /    \\                  |\n",
    "  |                /      \\                 |\n",
    "  |               /        \\                |\n",
    "  |            Mul()        Mul()           |\n",
    "  |            /   \\       /   \\            |\n",
    "  |  AccFetch()     |  Load(C)  \\           |\n",
    "  |                 |            \\          |\n",
    "  |       Load(alpha)           Load(beta)  |\n",
    "  |                                         |\n",
    "  +-----------------------------------------+\n",
    "            /         |         \\\n",
    "           /          |          \\              3. Individual kernel classes use the DAG representation\n",
    "          /           |           \\                to determine if the kernel class supports the DAG.\n",
    "    Kernel 0      Kernel 1     Kernel 2            If so, the kernel class emits DSL-level operations\n",
    "    epilogue      epilogue     epilogue            needed to compute the epilogue DAG alongside the\n",
    "    emitter       emitter      emitter             basic operation of the kernel (e.g., GEMM).\n",
    "       |              |            |\n",
    "       |              |            |\n",
    "       V              V            V\n",
    "```\n",
    "\n",
    "### Defining an epilogue via a Python function\n",
    "Epilogue fusion patterns are defined by users in Python functions that perform Tensor-level operations -- using a `torch.Tensor` (for example) resulting from matrix multiplication, the function must be able to compute the desired results of the epilogue.\n",
    "\n",
    "The structure of these functions is as follows:\n",
    "```python\n",
    "def custom_epi_name(accum, *args) -> Union[TensorType, tuple[TensorType]]:\n",
    "  \"\"\"\n",
    "  :param accum: result of matrix multiplication, convolution, etc. before the epilogue\n",
    "  :type accum: TensorType\n",
    "  :param args: additional arguments to be used in the epilogue (e.g., aux tensors)\n",
    "  :type args: list[Union[TensorType, ScalarType]]\n",
    "\n",
    "  :returns: at least one tensor resulting from the operation of the epilogue\n",
    "  :rtype: Union[TensorType, tuple[TensorType]]\n",
    "  \"\"\"\n",
    "  # Do some compute\n",
    "  return D # and potentially other values\n",
    "```\n",
    "\n",
    "The user defines a custom epilogue via a Python function that **must** do at least the following:\n",
    "1. Take in a first positional argument named `accum` that represents the result of operation just before the epilogue is to be performed. For example, in a GEMM, `accum = A @ B`.\n",
    "2. Return at least one tensor that results from computing the epilogue. Currently, the return list must contain at least one output named `D`, though this constraint may be loosened in the future.\n",
    "\n",
    "Each additional argument following `accum` in the function definition is expected to be either a Tensor or scalar to be loaded. Each variable in the return statement represents a Tensor or scalar to be stored. The underlying implementation of the epilogue in the kernel will determine how operands are loaded and stored.\n",
    "\n",
    "Compute operations are represented in static single assignment (SSA) form.\n",
    "This means that each variable can be assigned exactly once.\n",
    "Operations currently supported ares:\n",
    "* Tensor-tensor elementwise addition, subtraction, multiplication, and division\n",
    "* Scalar broadcasts via addition, subtraction, multiplication, and division\n",
    "* Predefined elementwise activation functions (e.g., ReLU, sigmoid, tanh)\n",
    "\n",
    "Operations that are not yet supported include:\n",
    "* Row/column broadcasts (planned to be added soon)\n",
    "* Reductions (planned to be added soon)\n",
    "* Binary minimum and maximum functions (planned to be added soon)\n",
    "If attempting to use these operations will result in no kernels being found in the call to `get_kernels`.\n",
    "\n",
    "Violations to SSA or use of unexpected operators will be flagged with an exception when parsing the AST of the custom epilogue.\n",
    "\n",
    "Examples of epilogues fitting these patterns are given below. We will show full, runnable examples at the end of this notebook.\n",
    "```python\n",
    "def relu_aux_store(accum, alpha, C):\n",
    "  # Note that the function definition itself does not indicate the types and\n",
    "  # ranks of alpha and C. Thus, one cannot tell whether the epilogue is performing\n",
    "  # broadcasts or elementwise operations until actual arguments or metadata are\n",
    "  # provided to the epilogue. See below for details.\n",
    "  F = (accum * alpha) + (C * 2.0) # Constant beta of 2.0\n",
    "  D = relu(F)\n",
    "  return D, F\n",
    "\n",
    "def aux_normalize(accum, aux):\n",
    "  D = accum / aux\n",
    "  return D\n",
    "```\n",
    "\n",
    "Additional information about each operand and output must be provided by the user when constructing `EpilogueArguments`, as we will discuss below. This additional information is necessary for fully defining the operations being performed -- without knowledge of whether `alpha` is a scalar or a Tensor, we cannot determine whether multiplication by `alpha` is a broadcasted or elementwise operation.\n",
    "\n",
    "### Constructing epilogue arguments\n",
    "`EpilogueArguments` encapsulate the arguments needed to determine the functional operation of a fused epilogue.\n",
    "\n",
    "A user must provide in the construction of `EpilogueArguments` tensors for all operands and outputs of the epilogue. However, unlike arguments for basic operations (e.g., GEMM), the full set of operands needed to be specified for an epilogue pattern depends upon the custom epilogue defined by the user.\n",
    "\n",
    "Therefore, `EpilogueArguments` is defined generically as taking in an `epilogue_fn` and additional `kwargs`. Under the hood, the AST for `epilogue_fn` is parsed to determine the operands and outputs of the epilogue. The user is required to provide in `kwargs` Tensors or scalars for all operands and outputs in the provided epilogue.\n",
    "\n",
    "For example, with an epilogue of:\n",
    "```python\n",
    "def my_epi(accum, alpha, C, beta):\n",
    "  F = (accum * alpha) + (C * beta)\n",
    "  D = relu(F)\n",
    "  return D, F\n",
    "```\n",
    "A user would need to construct epilogue arguments as follows:\n",
    "```python\n",
    "epi_args = EpilogueArguments(my_epi, alpha=..., C=..., beta=..., D=..., F=...)\n",
    "```\n",
    "\n",
    "After verifying that all required operands and outputs are present, the constructor to `EpilogueArguments` will perform additional passes on the AST of `epilogue_fn` using the provided inputs to generate an internal DAG representing the epilogue. This DAG structure is attached to `EpilogueArguments` for use as they are passed through a call to `get_kernels`.\n",
    "\n",
    "### Discovering kernels that support the epilogue pattern\n",
    "\n",
    "The call to `get_kernels(args)` will return any kernels that support the provided `GemmArguments`.\n",
    "Since the `GemmArguments` constructed above now include `EpilogueArguments`, returned kernels must support the provided epilogue.\n",
    "\n",
    "Under the hood of `get_kernels()`, each `Kernel` class will determine in its `generate_kernels()` method whether it supports the provided `EpilogueArguments`.\n",
    "It can do so by traversing the DAG that resulted from the construction of `EpilogueArguments` to find the operations that compose the epilogue.\n",
    "Assuming that the `Kernel` can support the DAG, it must then add to the source for the kernel any operations needed to support the DAG.\n",
    "An example of how this is done generically for an SM100 CuTe DSL GEMM is provided in `sm100_static_persistent_efc.py`.\n",
    "\n",
    "## Example epilogues\n",
    "We now provide various examples of adding custom epilogues to GEMM kernels targeting SM100. A broader set of epilogue examples are available in `test_gemm_epilogue_fusion.py`.\n",
    "\n",
    "### Auxiliary input and output tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ac178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cutlass_api.fusion.activation import relu\n",
    "\n",
    "def relu_aux_store(accum, alpha, C):\n",
    "  F = (accum * alpha) + (C * 2.0) # Constant beta\n",
    "  D = relu(F)\n",
    "  return D, F\n",
    "\n",
    "C = torch.randn((L, M, N), device=\"cuda\", dtype=torch.float16)\n",
    "alpha = 3.0\n",
    "D = torch.empty((L, M, N), device=\"cuda\", dtype=torch.float16)\n",
    "F = torch.empty((L, M, N), device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "epi_args = EpilogueArguments(relu_aux_store, alpha=alpha, C=C, D=D, F=F)\n",
    "args = GemmArguments(A=A, B=B, out=D, accumulator_type=torch.float32, epilogue=epi_args)\n",
    "kernels = cutlass_api.get_kernels(args, cc=100)\n",
    "assert len(kernels) > 0\n",
    "kernels[0].run(args)\n",
    "\n",
    "D_ref, F_ref = relu_aux_store(A @ B, alpha, C)\n",
    "\n",
    "torch.testing.assert_close(D, D_ref)\n",
    "torch.testing.assert_close(F, F_ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f947b403",
   "metadata": {},
   "source": [
    "### Keyword functions and returning accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_scale_return_acc(accum, alpha, beta, C, scale):\n",
    "  F = relu((accum * alpha) + (C * beta))\n",
    "  D = F * scale\n",
    "  return D, F, accum\n",
    "\n",
    "C = torch.randn((L, M, N), device=\"cuda\", dtype=torch.float16)\n",
    "alpha = 1.0\n",
    "beta = 2.0\n",
    "scale = 0.5\n",
    "D = torch.empty((L, M, N), device=\"cuda\", dtype=torch.float16)\n",
    "F = torch.empty((L, M, N), device=\"cuda\", dtype=torch.float16)\n",
    "accum = torch.empty((L, M, N), device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "epi_args = EpilogueArguments(relu_scale_return_acc, alpha=alpha, beta=beta, C=C, scale=scale, D=D, F=F, accum=accum)\n",
    "args = GemmArguments(A=A, B=B, out=D, accumulator_type=torch.float32, epilogue=epi_args)\n",
    "kernels = cutlass_api.get_kernels(args, cc=100)\n",
    "assert len(kernels) > 0\n",
    "kernels[0].run(args)\n",
    "\n",
    "D_ref, F_ref, accum_ref = relu_scale_return_acc(A @ B, alpha, beta, C, scale)\n",
    "\n",
    "torch.testing.assert_close(D, D_ref)\n",
    "torch.testing.assert_close(F, F_ref)\n",
    "torch.testing.assert_close(accum, accum_ref.to(accum.dtype))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641911f",
   "metadata": {},
   "source": [
    "### Passing a string representation of the function\n",
    "`EpilogueArguments` can additionally be constructed using a string representation of the epilogue function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_str = \"def epi(accum, alpha, beta, C): F = (accum * alpha) + (C * beta); D = relu(F); return D, F\"\n",
    "\n",
    "C = torch.randn((L, M, N), device=\"cuda\", dtype=torch.float16)\n",
    "alpha = 1.0\n",
    "beta = 0.5\n",
    "D = torch.empty((L, M, N), device=\"cuda\", dtype=torch.float16)\n",
    "F = torch.empty((L, M, N), device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "epi_args = EpilogueArguments(epi_str, alpha=alpha, beta=beta, C=C, D=D, F=F)\n",
    "args = GemmArguments(A=A, B=B, out=D, accumulator_type=torch.float32, epilogue=epi_args)\n",
    "kernels = cutlass_api.get_kernels(args, cc=100)\n",
    "assert len(kernels) > 0\n",
    "kernels[0].run(args)\n",
    "\n",
    "F_ref = (A @ B) * alpha + (C * beta)\n",
    "D_ref = torch.relu(F_ref)\n",
    "\n",
    "torch.testing.assert_close(D, D_ref)\n",
    "torch.testing.assert_close(F, F_ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a58a2",
   "metadata": {},
   "source": [
    "### Failure examples\n",
    "The following are examples of constructing `EpilogueArguments` that are expected to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Epilogues must take in an accumulator\n",
    "####################################################\n",
    "def fail_missing_accum(alpha, beta, C):\n",
    "  D = (C * beta)\n",
    "  return D\n",
    "\n",
    "try:\n",
    "  epi_args = EpilogueArguments(fail_missing_accum, alpha=alpha, beta=beta, C=C, D=D)\n",
    "  args = GemmArguments(A=A, B=B, out=D, accumulator_type=torch.float32, epilogue=epi_args)\n",
    "except Exception as e:\n",
    "  # \"accum must be an input to the epilogue function\"\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a359f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Epilogues must return an output named D\n",
    "####################################################\n",
    "def fail_missing_D(accum, alpha, beta, C):\n",
    "  F = (accum * alpha) + (C * beta)\n",
    "  return F\n",
    "\n",
    "try:\n",
    "  epi_args = EpilogueArguments(fail_missing_D, alpha=alpha, beta=beta, C=C, F=F)\n",
    "  args = GemmArguments(A=A, B=B, out=D, accumulator_type=torch.float32, epilogue=epi_args)\n",
    "except Exception as e:\n",
    "  # \"On SM90 or higher, D is expected to be a output node with 0 users to enable smem reuse between C and D, but got []\"\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Epilogues must use single-static assignment (SSA)\n",
    "####################################################\n",
    "def fail_ssa(accum):\n",
    "    tmp = accum * 2.0\n",
    "    # Redefine tmp, which violates SSA form.\n",
    "    tmp = tmp - 1.0\n",
    "    D = tmp / 4.0\n",
    "    return D, tmp\n",
    "\n",
    "try:\n",
    "  epi_args = EpilogueArguments(fail_ssa, D=D, tmp=F)\n",
    "  args = GemmArguments(A=A, B=B, out=D, accumulator_type=torch.float32, epilogue=epi_args)\n",
    "except Exception as e:\n",
    "  # \"Variable 'tmp' cannot be defined twice.\"\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871bb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Must provide all operands and outputs to\n",
    "# EpilogueArguments\n",
    "####################################################\n",
    "def my_epi(accum, alpha, beta, C):\n",
    "  F = (accum * alpha) + (C * beta)\n",
    "  D = relu(F)\n",
    "  return D\n",
    "\n",
    "try:\n",
    "  # Missing D\n",
    "  epi_args = EpilogueArguments(my_epi, alpha=alpha, beta=beta, C=C)\n",
    "  args = GemmArguments(A=A, B=B, out=D, accumulator_type=torch.float32, epilogue=epi_args)\n",
    "except Exception as e:\n",
    "  # \"Argument D is not provided in the kwargs of the EpilogueArguments constructor\"\n",
    "  print(e)\n",
    "\n",
    "try:\n",
    "  # Missing alpha\n",
    "  epi_args = EpilogueArguments(my_epi, beta=beta, C=C, D=D)\n",
    "  args = GemmArguments(A=A, B=B, out=D, accumulator_type=torch.float32, epilogue=epi_args)\n",
    "except Exception as e:\n",
    "  # \"Argument alpha is not provided in the kwargs of the EpilogueArguments constructor\"\n",
    "  print(e)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
