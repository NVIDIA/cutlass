{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd45ef2",
   "metadata": {},
   "source": [
    "# Basic GEMM using CUTLASS Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709aa60",
   "metadata": {},
   "source": [
    "The CUTLASS API provides a consistent, uniform interface for discovering, compiling, and running GPU kernels from various DSL sources.\n",
    "\n",
    "This notebook walks through a minimal GEMM (Generalized Matrix-Matrix Multiplication) example, and introduces the core concepts of the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f878d960-d175-4d84-b978-88afbd318850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import cutlass\n",
    "\n",
    "import cutlass_api\n",
    "\n",
    "if not (status := cutlass_api.utils.is_device_cc_supported({100, 103})):\n",
    "    print(\n",
    "        f\"This notebook requires a GPU with compute capability 100 or 103.\\n{status.error}\"\n",
    "    )\n",
    "    import sys\n",
    "\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91dab6",
   "metadata": {},
   "source": [
    "## Running your first kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b87b0",
   "metadata": {},
   "source": [
    "### Setting up arguments\n",
    "\n",
    "CUTLASS API has first-class support for PyTorch tensors. We start by creating torch tensors that will be operands to a matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f550c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, K, L = 128, 256, 64, 2\n",
    "ab_type = torch.float16\n",
    "out_type = torch.float32\n",
    "acc_type = torch.float32\n",
    "\n",
    "A = torch.randint(-1, 2, (L, M, K), device=\"cuda\", dtype=ab_type)\n",
    "B = torch.randint(-1, 2, (L, K, N), device=\"cuda\", dtype=ab_type)\n",
    "out = torch.empty((L, M, N), device=\"cuda\", dtype=out_type)\n",
    "\n",
    "reference = (A @ B).to(out.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cb805",
   "metadata": {},
   "source": [
    "We then create a `GemmArguments` object. This object specifies:\n",
    "1. what logical operation do we want to perform (a GEMM)\n",
    "2. on which operands we want to perform that operation (`A`, `B`, `out` as declared above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b57690df",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = cutlass_api.arguments.GemmArguments(A=A, B=B, out=out, accumulator_type=acc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5ddcf",
   "metadata": {},
   "source": [
    "### Kernel discovery\n",
    "\n",
    "We now need to find kernels that can perform the operation we expressed in `args`.\n",
    "\n",
    "The simplest way to do so is to use `get_kernels(args)`. It searches a set of kernels pre-registered in the library, and returns the subset of those kernels which can successfully run our given `args`.\n",
    "\n",
    "Any of these kernels will be functionally equivalent -- they may have different design or performance characteristics. We arbitrarily pick the first of the returned kernels to execute here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9872ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = cutlass_api.get_kernels(args)\n",
    "assert kernels, \"No kernels found for the given arguments!\"\n",
    "\n",
    "kernel = kernels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17693e",
   "metadata": {},
   "source": [
    "#### Run the kernel\n",
    "\n",
    "Running the kernel is as simple as `kernel.run(args)`.\n",
    "\n",
    "This implicitly JIT-compiles the kernel, and launches it on the GPU device using our given arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf4588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.run(args)\n",
    "\n",
    "torch.testing.assert_close(out, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ad85b",
   "metadata": {},
   "source": [
    "One can also explicitly compile the kernel and pass this in to `kernel.run` to avoid\n",
    "JIT compilation on future invocations. Additional details related to this will be\n",
    "described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = kernel.compile(args)\n",
    "kernel.run(args, compiled_artifact=artifact)\n",
    "torch.testing.assert_close(out, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e9e4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding the core interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b8e94",
   "metadata": {},
   "source": [
    "#### 1. `RuntimeArguments` / `GemmArguments`\n",
    "\n",
    "`RuntimeArguments` describe the operation a user wants to perform, and all the runtime operands or other runtime parameters needed for it. \n",
    "This includes primary runtime operands to the operation, as well as any custom epilogue fusions and runtime performance knobs.\n",
    "\n",
    "We provide builtin subtypes of `RuntimeArguments` for common operations (e.g. GEMM, Elementwise ops; more later).\n",
    "\n",
    "For instance, `GemmArguments` is a type of `RuntimeArguments`:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class GemmArguments(RuntimeArguments):\n",
    "    A: TensorLike\n",
    "    B: TensorLike\n",
    "    out: TensorLike\n",
    "    accumulator_type: NumericLike\n",
    "```\n",
    "\n",
    "`GemmArguments` conveys:\n",
    "* We want to perform a dense GEMM operation (`out = A @ B`)\n",
    "* We want to perform it for operands in `A, B, out`, with intermediate results stored as `accumulator_type`\n",
    "* We can optionally set a custom epilogue that is fused on top of the base GEMM. Some kernels also support some runtime performance controls which can be specified here. These will be discussed in detail in other tutorials.\n",
    "\n",
    "It is a kernel-agnostic way to specify the desired functionality.\n",
    "\n",
    "`RuntimeArguments` can be constructed from any `TensorLike` object. This includes `torch.Tensor`, `cute.Tensor`, or any other DLPack-compatible tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eda0dd",
   "metadata": {},
   "source": [
    "#### 2. Kernel Discovery\n",
    "\n",
    "There are several kernels available in CUTLASS DSLs that are registered with, and discoverable via, the CUTLASS API.\n",
    "\n",
    "This includes kernels for various operations (GEMM, Elementwise operations, ...), which implement various algorithms & architecture features. Within the same implementation, there are several instances or  configurations of it with different combinations of operand types, layouts, tile sizes, etc.\n",
    "\n",
    "In the previous step, we used `GemmArguments` to specify our desired GEMM in a kernel-agnostic way. Now we find kernels that can fulfill that functionality. A subset of the available kernels will perform GEMM, and a subset of _those_ will support the properties of specific operands we are currently using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b737131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 107616 kernel instances are available.\n",
      "Of these, 350 support the given arguments.\n",
      "Picked kernel with name: cutedsl.PersistentDenseGemmKernel_sm100_ttt_AFloat16_BFloat16_outFloat32_accFloat32_2cta_cluster2x1x1_tile128x32x256_tma_store\n"
     ]
    }
   ],
   "source": [
    "# get_kernels() fetches all kernels when called without args\n",
    "all_kernels = cutlass_api.get_kernels()\n",
    "print(f\"A total of {len(all_kernels)} kernel instances are available.\")\n",
    "\n",
    "# we can limit the search to kernels supporting given args\n",
    "kernels = cutlass_api.get_kernels(args)\n",
    "print(f\"Of these, {len(kernels)} support the given arguments.\")\n",
    "\n",
    "kernel = kernels[0]\n",
    "print(f\"Picked kernel with name: {kernel.metadata.kernel_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a4d38",
   "metadata": {},
   "source": [
    "#### 3. `Kernel` execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d004b",
   "metadata": {},
   "source": [
    "Once we have selected a kernel, we are now ready to execute it. We previously showed the simplest way to do this is `kernel.run(args)`.\n",
    "\n",
    "This method does the following:\n",
    "* verify that the kernel supports the given `args`\n",
    "* JIT-compile the kernel\n",
    "* launch the compiled kernel function\n",
    "\n",
    "Users can do these steps individually for more control:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8945aa6",
   "metadata": {},
   "source": [
    "* `kernel.supports(args)` checks if the kernel supports the given `args`\n",
    "    * this is relevant if the kernel was not picked just for these `args`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159fd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "supported = kernel.supports(args)\n",
    "assert supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948689a8",
   "metadata": {},
   "source": [
    "If the arguments are not supported by this kernel, `supports` returns a `Status` object explaining the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cfc9ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operand `A` is unsupported: Expected element type Float16, got BFloat16\n"
     ]
    }
   ],
   "source": [
    "unsupported_args = cutlass_api.arguments.GemmArguments(\n",
    "    A=A.to(torch.bfloat16), B=B, out=out, accumulator_type=acc_type\n",
    ")\n",
    "if not (status := kernel.supports(unsupported_args)):\n",
    "    print(status.error)\n",
    "\n",
    "assert not status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db8f20",
   "metadata": {},
   "source": [
    "* `kernel.compile(args)` compiles the kernel, and returns a `CompiledArtifact`\n",
    "\n",
    "This compiled artifact is a lightweight wrapper over the result of compiling a kernel (e.g., via `cute.compile()`).\n",
    "\n",
    "For just-in-time compilation, we can use the compiled artifact straightaway.\n",
    "In the future, we will support optionally serializing it for ahead-of-time compilation and deserialized in a different context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e79eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_artifact = kernel.compile(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb8d51",
   "metadata": {},
   "source": [
    "* `kernel.run(args)` launches the compiled kernel function. This example uses:\n",
    "    * the precompiled artifact\n",
    "    * a custom stream to launch to\n",
    "    * bypasses the supports check already performed above (`assume_supported_args=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02398bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero the output to avoid testing stale output\n",
    "out.zero_()\n",
    "\n",
    "kernel.run(\n",
    "    args,\n",
    "    compiled_artifact,\n",
    "    stream=torch.cuda.Stream(),\n",
    "    assume_supported_args=True,\n",
    ")\n",
    "torch.testing.assert_close(out, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67eeb8f",
   "metadata": {},
   "source": [
    "Some kernels may also require a device \"workspace\". This is an additional buffer needed by some kernels for book-keeping, temporary results, etc.\n",
    "Its size can be queried using `kernel.get_workspace_size(args)`. Most kernels will have a workspace size of 0.\n",
    "If a kernel does have a non-zero workspace size, an additional buffer of at least that size must be provided. Without it, the kernel behavior is undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b2e2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_size = kernel.get_workspace_size(args)\n",
    "workspace = torch.empty(workspace_size, device=\"cuda\", dtype=torch.int8)\n",
    "\n",
    "out.zero_()\n",
    "kernel.run(args, compiled_artifact, stream=torch.cuda.Stream(), workspace=workspace)\n",
    "torch.testing.assert_close(out, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baffaf12",
   "metadata": {},
   "source": [
    "### Advanced: Filtering on Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd521a",
   "metadata": {},
   "source": [
    "Using `RuntimeArguments` to search for supporting kernels is a convenient way to discover kernels: users directly specify their desired functionality, and `get_kernels()` finds the supporting kernels.\n",
    "It covers all logical operands of any operation, as well as (in later examples) epilogue fusions, and performance controls.\n",
    "\n",
    "However, there may be cases where users want more advanced ways to query kernels. These could be:\n",
    "* when the desired properties may not be expressed in runtime controls\n",
    "   * the simplest scenario may be if you're searching searching for a kernel with a specific name, a specific class, etc.\n",
    "   * searching for kernel's static properties such as tile size, cluster size, etc.\n",
    "* when the `RuntimeArguments` are not available or you want to generate & pre-compile a broader set of kernels\n",
    "\n",
    "For such cases, we provide a more advanced filtering based on `KernelMetadata`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54ae50",
   "metadata": {},
   "source": [
    "`KernelMetadata` captures a wide variety of properties of a `Kernel`.\n",
    "\n",
    "These are properties of a kernel's functional support (like operand types, layouts, alignments), as well as architectural/design choices & performance characteristics (like tilze size, scheduling characteristics).\n",
    "\n",
    "Different kernels may use different sub-classes of `metadata.operands`, `metadata.design`, `metadata.epilogue` for flexibility, which can also identify their characteristics.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class KernelMetadata:\n",
    "    kernel_name: str\n",
    "    kernel_class: type[\"Kernel\"]\n",
    "    min_cc: int\n",
    "    operands: OperandsMetadata\n",
    "    design: DesignMetadata | None = None\n",
    "    epilogue: EpilogueMetadata | None = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9943888",
   "metadata": {},
   "source": [
    "Every unique kernel instance can be distinguished by its metadata.\n",
    "It can be used in filtering for kernels in addition to the `RuntimeArguments`, by providing a custom `metadata_filter`.\n",
    "\n",
    "Here, we get all kernels that support `args`, and have `metadata.design` of type `Sm100DesignMetadata`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8717ac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 350 kernels which support args & have Sm100DesignMetadata\n"
     ]
    }
   ],
   "source": [
    "kernels = cutlass_api.get_kernels(\n",
    "    args,\n",
    "    metadata_filter=lambda metadata: isinstance(\n",
    "        metadata.design, cutlass_api.metadata.Sm100DesignMetadata\n",
    "    ),\n",
    ")\n",
    "print(f\"Found {len(kernels)} kernels which support args & have Sm100DesignMetadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f9124",
   "metadata": {},
   "source": [
    "We can construct more advanced filters by leveraging duck-typing.\n",
    "Additionally, we can get all the kernels that match our filter, rather than supporting a fully-defined set of arguments.\n",
    "This could be useful to pre-generate large set of kernels not targeted to any one problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a76ec20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9400 matching kernels\n"
     ]
    }
   ],
   "source": [
    "def a_more_complex_filter(metadata: cutlass_api.metadata.KernelMetadata) -> bool:\n",
    "    \"\"\"\n",
    "    Find all GEMM kernels that support Float16 A and 2-CTA MMA\n",
    "    \"\"\"\n",
    "    # Only look at GEMM kernels\n",
    "    if not isinstance(metadata.operands, cutlass_api.metadata.GemmOperandsMetadata):\n",
    "        return False\n",
    "    # Only look at kernels with A-type F16\n",
    "    if metadata.operands.A.dtype != cutlass.Float16:\n",
    "        return False\n",
    "    # Only look at kernels with tile_shape[0] == 128\n",
    "    if getattr(metadata.design, \"tile_shape\", [None])[0] != 128:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Look ma, no args! Fetch all kernels that match the filter,\n",
    "# instead of supporting a complete set of args\n",
    "kernels = cutlass_api.get_kernels(\n",
    "    args=None,\n",
    "    metadata_filter=a_more_complex_filter,\n",
    ")\n",
    "print(f\"Found {len(kernels)} matching kernels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
